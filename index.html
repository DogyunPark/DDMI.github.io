<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="INR, latent diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-quality Implicit Neural Representations</title>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-quality Implicit Neural Representations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Dogyun Park</a>,</span>
            <span class="author-block">
              Sihyeon Kim</a>,</span>
            <span class="author-block">
              Sojin Lee</a>,
            </span>
            <span class="author-block">
              <a href="https://pages.cs.wisc.edu/~hwkim/">Hyunwoo J. Kim</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mlv.korea.ac.kr/home/">MLV LAB</a>,
            </span>
            <span class="author-block">Korea University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              ICLR 2024</a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        TL;DR: We learn a latent diffusion models that generates hierarchically decomposed positional embeddings of signals, enabling high-quality generation on various signal domains.
      </h2>
      <img src="static/images/mainfigure.png" alt="Additional Results" class="center-image blend-img-background"/>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains.
            These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation.
            We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs).
            Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation.
            To address this limitation, we propose <b style="color: #000000">D</b>omain-agnostic Latent <b style="color: #000000">D</b>iffusion <b style="color: #000000">M</b>odel for <b style="color: #000000">I</b>NRs (<b style="color: #000000">DDMI</b>) that generates adaptive positional embeddings instead of neural networks' weights.
            Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (<b style="color: #000000">D2C-VAE</b>), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. 
            Additionally, we introduce a novel conditioning mechanism for evaluating INRs with the hierarchically decomposed PEs to further enhance expressive power.
            Extensive experiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance Fields, and videos, with seven benchmark datasets, demonstrate the versatility of DDMI and its superior performance compared to the existing INR generative models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
<hr>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Overview. -->
    
    <div class="content">
      <h2 class="title is-3">Method Overview</h2>
      <h3 class="title is-4">PE generation</h3>
      <p>
        To generate Implicit neural representation, we propose generating adaptive positional embeddings instead of the weight of INRs. 
        This shifts the primary expressive power from MLP to PE, which we have observed to lead to more fine-detailed generation results. 
        To further enhance the expressive capacity, we hierarchically decompose the PEs (HDBFs) and modulate the MLPs in a course-to-fine manner (CFC).
      </p>
      <img src="static/videos/concept.gif" alt="Framework" class="center-image blend-img-background"/>
      <h3 class="title is-4">Two-stage training</h3>
      <p>
        In the first stage, we learn the latent space of continuous signals via our D2C-VAE framework.
        Then, we approximate the distribution of latent space with the latent diffusion model (DDMI). 
        After training, DDMI generate hierarchically decomposed positional embeddings (HDBFs), where the MLPs read out HDBFs into the signal values.
      </p>
      <img src="static/videos/framework.gif" alt="Framework" class="center-image blend-img-background"/>
    </div>
  </div>
<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">2D Image Results</h2>

        <!-- unconditional generation. -->
        <h3 class="title is-4">Unconditional generation</h3>
          <div class="content has-text-centered">
            <img src="static/images/image_compare.jpg" alt="Framework" width="100%" class="center blend-img-background"/>
          </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Arbitrary-scale image generation</h3>
          <p>
            With DDMI, we can freely control the scale of images, e.g., arbitrary resolution or zoom-in, by querying different sets of coordinates c.
          </p>
        <div class="content has-text-centered">
          <img src="static/videos/arbitrary_generation3.gif" alt="arbitrary_generation" class="blend-img-background"/>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">3D Shape Generation Results</h2>

        <!-- unconditional generation. -->
        <h3 class="title is-4">Unconditional</h3>
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item item-steve">
                <video poster="" id="steve" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/obj2.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-truck">
                <video poster="" id="truck" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/truck.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-longsofa">
                <video poster="" id="longsofa" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/longsofa.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-roundtable">
                <video poster="" id="roundtable" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/roundtable.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-ship-tp">
                <video poster="" id="ship" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/ship.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-table-tp">
                <video poster="" id="table" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/table.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-table2-tp">
                <video poster="" id="table2" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/table2.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-rifle-tp">
                <video poster="" id="rifle" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/rifle.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-car-tp">
                <video poster="" id="car" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/car.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-bench-tp">
                <video poster="" id="bench" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/bench.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-toolchair-tp">
                <video poster="" id="toolchair" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/toolchair.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-airplane-tp">
                <video poster="" id="airplane" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/airplane.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-supercar-tp">
                <video poster="" id="supercar" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/supercar.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-clinechair-tp">
                <video poster="" id="clinechair" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/clinechair.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-chair-tp">
                <video poster="" id="chair" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/chair.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-van-tp">
                <video poster="" id="van" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/van.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Text-to-shape</h3>
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">2D Video Generation Results</h2>

        <!-- unconditional generation. -->
        <h3 class="title is-4">Unconditional</h3>
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
          <img src="static/images/image_compare.jpg" alt="Framework" width="100%" class="center blend-img-background"/>
        <br/>
        <!--/ Interpolating. -->
      </div>
    </div>
    <!--/ Animation. -->
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
